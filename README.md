
# ðŸ“Š Data Analyst AI - Automated Data Analysis Platform

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104.1-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

**Data Analyst AI** is an intelligent, AI-powered platform that automates 80-90% of a data analyst's workflow. From data cleaning to insight generation and predictive modeling, this system provides comprehensive data analysis capabilities through a powerful API.

## ðŸš€ Features

### Core Capabilities
- **ðŸ“ Smart Data Ingestion** - Support for CSV, Excel, JSON files
- **ðŸ§¹ Automated Data Cleaning** - Handle missing values, outliers, duplicates
- **ðŸ“Š Exploratory Data Analysis (EDA)** - Automatic statistical analysis and correlation detection
- **ðŸ¤– AI-Powered Insights** - GPT-4 powered natural language insights and recommendations
- **ðŸ“ˆ Automatic Visualization** - Smart chart generation based on data types
- **ðŸ”® Predictive Modeling** - AutoML for classification, regression, and clustering
- **ðŸ’¬ Chat with Your Data** - Natural language queries about your datasets
- **ðŸ“‘ Report Generation** - Export analysis to PDF, Excel, PowerPoint

### Advanced Features
- **Real-time Data Processing**
- **RESTful API**
- **Scalable Architecture**
- **File Persistence**
- **Model Storage**

## ðŸ—ï¸ System Architecture

```
Frontend (React/Streamlit) â†â†’ FastAPI Backend â†â†’ AI Engine
       â†‘                             â†‘                 â†‘
   User Interface              Business Logic     AI/ML Processing
                               Data Processing    Insight Generation
```


## ðŸ› ï¸ Tech Stack

### Backend
- **Framework**: FastAPI (Python)
- **Data Processing**: Pandas, NumPy
- **Machine Learning**: Scikit-learn
- **AI/LLM**: OpenAI GPT-4
- **Visualization**: Plotly, Matplotlib
- **File Processing**: ReportLab

### Infrastructure
- **Database**: SQLite (with PostgreSQL support)
- **File Storage**: Local File System
- **Authentication**: JWT Tokens (ready for implementation)

## ðŸ“ Current Project Structure
```
data-analyst-ai/  
â”œâ”€â”€ .gitignore  
â”œâ”€â”€Â [README.md](https://readme.md/)  
â”œâ”€â”€ requirements.txt  
â””â”€â”€ server/  
â”œâ”€â”€ .gitignore  
â”œâ”€â”€ config.py  
â”œâ”€â”€ data/  
â”‚ â”œâ”€â”€ models/ # Trained ML models (.pkl files)  
â”‚ â”œâ”€â”€ processed/ # Cleaned datasets  
â”‚ â”œâ”€â”€ raw/ # Original uploaded files  
â”‚ â””â”€â”€ reports/ # Generated reports  
â”œâ”€â”€ database/  
â”‚ â”œâ”€â”€ db_connection.py  
â”‚ â””â”€â”€ user_table.py  
â”œâ”€â”€ main.py # FastAPI application entry point  
â”œâ”€â”€ models/  
â”‚ â””â”€â”€ dataset_schema.py # Pydantic schemas  
â”œâ”€â”€ routers/ # API endpoints  
â”‚ â”œâ”€â”€ data_cleaning.py  
â”‚ â”œâ”€â”€ data_upload.py  
â”‚ â”œâ”€â”€ eda.py  
â”‚ â”œâ”€â”€ insights.py  
â”‚ â”œâ”€â”€ modeling.py  
â”‚ â”œâ”€â”€ reports.py  
â”‚ â””â”€â”€ visualization.py  
â”œâ”€â”€ services/ # Business logic  
â”‚ â”œâ”€â”€ data_cleaner.py  
â”‚ â”œâ”€â”€ data_loader.py  
â”‚ â”œâ”€â”€ eda_engine.py  
â”‚ â”œâ”€â”€ file_manager.py  
â”‚ â”œâ”€â”€ insight_generator.py  
â”‚ â”œâ”€â”€ model_trainer.py  
â”‚ â”œâ”€â”€ report_generator.py  
â”‚ â””â”€â”€ visualization_engine.py  
â””â”€â”€ utils/ # Helper functions  
â”œâ”€â”€ file_utils.py  
â”œâ”€â”€ logger.py  
â””â”€â”€ time_utils.py
```

## ðŸ“ Project Structure

```
data-analyst-ai/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”‚
â”œâ”€â”€ data/                         # Sample & uploaded datasets
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ reports/
â”‚
â”œâ”€â”€ server/                      # FastAPI backend (core logic)
â”‚   â”œâ”€â”€ main.py                   # Entry point (FastAPI app)
â”‚   â”œâ”€â”€ config.py                 # Environment variables, API keys
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ routers/                  # All API endpoints grouped by feature
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_upload.py        # Handles CSV/XLS upload
â”‚   â”‚   â”œâ”€â”€ data_cleaning.py      # Trigger cleaning pipeline
â”‚   â”‚   â”œâ”€â”€ eda.py                # Runs automatic EDA
â”‚   â”‚   â”œâ”€â”€ visualization.py      # Returns plots
â”‚   â”‚   â”œâ”€â”€ insights.py           # LLM-generated insights
â”‚   â”‚   â”œâ”€â”€ modeling.py           # Predictive ML tasks
â”‚   â”‚   â””â”€â”€ reports.py            # PDF/Excel export routes
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                 # Business logic & AI modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”‚   â”œâ”€â”€ data_cleaner.py
â”‚   â”‚   â”œâ”€â”€ eda_engine.py
â”‚   â”‚   â”œâ”€â”€ visualization_engine.py
â”‚   â”‚   â”œâ”€â”€ insight_generator.py
â”‚   â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”‚   â”œâ”€â”€ report_generator.py
â”‚   â”‚   â””â”€â”€ file_manager.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                    # Helper functions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â”œâ”€â”€ decorators.py
â”‚   â”‚   â”œâ”€â”€ file_utils.py
â”‚   â”‚   â””â”€â”€ time_utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                   # Data models (schemas, Pydantic)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset_schema.py
â”‚   â”‚   â”œâ”€â”€ user_schema.py
â”‚   â”‚   â”œâ”€â”€ eda_schema.py
â”‚   â”‚   â””â”€â”€ insight_schema.py
â”‚   â”‚
â”‚   â””â”€â”€ database/                 # (optional) Database logic
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ db_connection.py
â”‚       â”œâ”€â”€ user_table.py
â”‚       â””â”€â”€ dataset_table.py
â”‚
â”œâ”€â”€ client/                     # UI layer
â”‚   â”œâ”€â”€ streamlit_app.py          # MVP version (Streamlit)
â”‚   â”‚
â”‚   â”œâ”€â”€ react_app/                # Advanced UI version
â”‚   â”‚   â”œâ”€â”€ package.json
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ index.js
â”‚   â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ UploadFile.jsx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.jsx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ InsightsPanel.jsx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatWithData.jsx
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ReportViewer.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Home.jsx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Login.jsx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Explore.jsx
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Predict.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ api.js
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ auth.js
â”‚   â”‚   â”‚   â””â”€â”€ styles/
â”‚   â”‚   â”‚       â””â”€â”€ main.css
â”‚   â”‚   â””â”€â”€ public/
â”‚   â”‚       â”œâ”€â”€ index.html
â”‚   â”‚       â””â”€â”€ favicon.ico
â”‚
â”œâ”€â”€ ai_engine/                    # Core analytics intelligence
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocess.py             # Cleaning + missing value handling
â”‚   â”œâ”€â”€ analyzer.py               # EDA + correlation analysis
â”‚   â”œâ”€â”€ visualizer.py             # Automatic chart generation
â”‚   â”œâ”€â”€ modeler.py                # AutoML + forecasting
â”‚   â”œâ”€â”€ insight_agent.py          # LLM-based natural language insights
â”‚   â”œâ”€â”€ chat_agent.py             # â€œChat with your dataâ€ logic
â”‚   â”œâ”€â”€ report_builder.py         # Summaries + chart embedding
â”‚   â””â”€â”€ prompt_templates/         # LLM prompt templates
â”‚       â”œâ”€â”€ eda_summary.txt
â”‚       â”œâ”€â”€ correlation_summary.txt
â”‚       â””â”€â”€ forecast_summary.txt
â”‚
â”œâ”€â”€ tests/                        # Unit + integration tests
â”‚   â”œâ”€â”€ test_api_endpoints.py
â”‚   â”œâ”€â”€ test_data_cleaning.py
â”‚   â”œâ”€â”€ test_visualizations.py
â”‚   â”œâ”€â”€ test_insight_generation.py
â”‚   â””â”€â”€ test_model_training.py
â”‚
â”œâ”€â”€ scripts/                      # Utility scripts for dev/deploy
â”‚   â”œâ”€â”€ run_local.sh
â”‚   â”œâ”€â”€ deploy_aws.sh
â”‚   â””â”€â”€ init_db.py
â”‚
â””â”€â”€ docs/                         # Documentation
    â”œâ”€â”€ architecture-diagram.png
    â”œâ”€â”€ api_docs.md
    â”œâ”€â”€ system_design.md
    â”œâ”€â”€ data_flow.md
    â””â”€â”€ roadmap.md
```

## ðŸš€ Quick Start

### Prerequisites
- Python 3.8+
- OpenAI API key (for AI features)


### Installation

1. **Clone the repository**
```
git clone https://github.com/yourusername/data-analyst-ai.git
cd data-analyst-ai

```
2. **Backend Setup**
```
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set environment variables
echo "OPENAI_API_KEY=your-openai-api-key-here" > .env
echo "DEBUG=True" >> .env
echo "SECRET_KEY=your-secret-key-here" >> .env
```

3. **Frontend Setup (React)**
```
npm run dev
```

3. **Run the Application**
```
cd server
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

4. **Run the Application**
```
# Start backend server
cd server
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Start frontend (in separate terminal)
cd client/react_app
npm start

```

## ðŸ“š API Documentation

Once the server is running, access the interactive API documentation:

- **Swagger UI**:Â [http://localhost:8000/docs](http://localhost:8000/docs)
    
- **ReDoc**:Â [http://localhost:8000/redoc](http://localhost:8000/redoc)
    

### Key API Endpoints

|Endpoint|Method|Description|
|---|---|---|
|`/api/v1/upload`|POST|Upload dataset files|
|`/api/v1/clean`|POST|Data cleaning operations|
|`/api/v1/analyze`|POST|Perform EDA|
|`/api/v1/visualize/generate`|POST|Generate visualizations|
|`/api/v1/insights/generate`|POST|AI-powered insights|
|`/api/v1/models/train`|POST|Train ML models|
|`/api/v1/reports/generate`|POST|Generate reports|
|`/api/v1/insights/chat`|POST|Chat with your data|

## ðŸ”§ Configuration

### Environment Variables

Create aÂ `.env`Â file in the root directory:

```
# API Configuration
DEBUG=True
SECRET_KEY=your-secret-key-here

# AI Services
OPENAI_API_KEY=your-openai-api-key
OPENAI_MODEL=gpt-4

# File Upload
MAX_FILE_SIZE=104857600  # 100MB in bytes

# Database
DATABASE_URL=sqlite:///./data_analyst_ai.db
```

### Supported File Formats

- **CSV**Â (.csv)
    
- **Excel**Â (.xlsx, .xls)
    
- **JSON**Â (.json)
    

## ðŸ“Š Performance

- **File Processing**: Handles datasets up to 100MB
    
- **Response Time**: < 5 seconds for most operations
    
- **Concurrent Users**: Supports 50+ simultaneous users
    
- **AI Processing**: Optimized for GPT-4 API usage
    

## ðŸ¤ Contributing

We welcome contributions! Please see ourÂ [Contributing Guide](https://contributing.md/)Â for details.

1. Fork the repository
    
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
    
3. Commit your changes (`git commit -m 'Add amazing feature'`)
    
4. Push to the branch (`git push origin feature/amazing-feature`)
    
5. Open a Pull Request
    

## ðŸ“„ License

This project is licensed under the MIT License - see theÂ [LICENSE](https://license/)Â file for details.

## ðŸ†˜ Support

- **Documentation**:Â [GitHub Wiki](https://github.com/yourusername/data-analyst-ai/wiki)
    
- **Issues**:Â [GitHub Issues](https://github.com/yourusername/data-analyst-ai/issues)
    
- **Email**:Â [support@data-analyst-ai.com](https://mailto:support@data-analyst-ai.com/)
    

## ðŸ™ Acknowledgments

- OpenAI for GPT-4 API
    
- FastAPI team for the excellent web framework
    
- Pandas and Scikit-learn communities
    
- Plotly for interactive visualizations
    

---

**Data Analyst AI**Â - Making data analysis accessible to everyone! ðŸš€

_For more information, visit ourÂ [documentation](https://github.com/Aryan-202/data-analyst-ai/wiki)Â or join ourÂ [community discussions](https://github.com/Aryan-202/data-analyst-ai/discussions)._